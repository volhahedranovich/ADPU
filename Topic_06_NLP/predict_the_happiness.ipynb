{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the Happiness\n",
    "\n",
    "* [competition url](https://www.hackerearth.com/problem/machine-learning/predict-the-happiness/)\n",
    "* [fasttext](https://github.com/facebookresearch/fastText)\n",
    "\n",
    "**Task:** solve sentiment analysis problem (text classification). Classify TripAdvisor revies as \"happy\" or \"not happy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import (train_test_split,\n",
    "                                     StratifiedKFold,\n",
    "                                     GridSearchCV)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals.joblib import Memory\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/ubuntu/train.csv')\n",
    "df.drop(['User_ID'], 1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df['Is_Response'] = LabelBinarizer().fit_transform(df['Is_Response'].values)\n",
    "df['Browser_Used'] = LabelEncoder().fit_transform(df['Browser_Used'].values)\n",
    "df['Device_Used'] = LabelEncoder().fit_transform(df['Device_Used'].values)\n",
    "df['Description'] = df['Description'].map(lambda x: x.replace('\\n', ' '))\n",
    "\n",
    "# an exta collumn for fasttext\n",
    "df['y'] = df['Is_Response'].map(lambda x: '__label__{}'.format(x))\n",
    "\n",
    "train_df, valid_df = train_test_split(df,\n",
    "                                     test_size=0.2,\n",
    "                                     stratify=df['Is_Response'].values,\n",
    "                                     random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Browser_Used</th>\n",
       "      <th>Device_Used</th>\n",
       "      <th>Is_Response</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15200</th>\n",
       "      <td>We stayed prior to a cruise. Since we had just...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>__label__0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27682</th>\n",
       "      <td>The hotel rooms were under renovation and the ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20617</th>\n",
       "      <td>Front desk staff were excellent, actively look...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>__label__0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28803</th>\n",
       "      <td>Spent a wonderful night at the Amalfi with fri...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>__label__0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25801</th>\n",
       "      <td>The hotel was clean, room was clean, the conti...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Description  Browser_Used  \\\n",
       "15200  We stayed prior to a cruise. Since we had just...             2   \n",
       "27682  The hotel rooms were under renovation and the ...             2   \n",
       "20617  Front desk staff were excellent, actively look...             1   \n",
       "28803  Spent a wonderful night at the Amalfi with fri...             6   \n",
       "25801  The hotel was clean, room was clean, the conti...             1   \n",
       "\n",
       "       Device_Used  Is_Response           y  \n",
       "15200            0            0  __label__0  \n",
       "27682            2            1  __label__1  \n",
       "20617            1            0  __label__0  \n",
       "28803            1            0  __label__0  \n",
       "25801            1            1  __label__1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_df.drop('Is_Response', 1), train_df['Is_Response'].values\n",
    "X_valid, y_valid = valid_df.drop('Is_Response', 1), valid_df['Is_Response'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column, reshape=False):\n",
    "        self.column = column\n",
    "        self.reshape = reshape\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X =  X[self.column].values\n",
    "        if self.reshape:\n",
    "            X = X.reshape(-1, 1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 15000\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('[^\\w]', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "browser_pipe = make_pipeline(ColSelector('Browser_Used', True), OneHotEncoder())\n",
    "device_pipe = make_pipeline(ColSelector('Device_Used', True), OneHotEncoder())\n",
    "text_pipe = make_pipeline(ColSelector('Description'), \n",
    "                          TfidfVectorizer(max_features=VOCAB_SIZE,\n",
    "                                          stop_words=en_stopwords,\n",
    "                                          tokenizer=word_tokenize,\n",
    "                                          min_df=5,\n",
    "                                          sublinear_tf=True,\n",
    "                                          preprocessor=preprocessor,\n",
    "                                          ngram_range=(1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('pipeline-1', Pipeline(memory=None,\n",
       "     steps=[('colselector', ColSelector(column='Browser_Used', reshape=True)), ('onehotencoder', OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True))])), ('pipelin...   tokenizer=<function word_tokenize at 0x7f6f34a08c80>, use_idf=True,\n",
       "        vocabulary=None))]))],\n",
       "       transformer_weights=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extraction_pipeline = make_union(browser_pipe, device_pipe, text_pipe)\n",
    "feature_extraction_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = feature_extraction_pipeline.transform(X_train)\n",
    "X_valid_features = feature_extraction_pipeline.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV] n_estimators=10, max_depth=20 ...................................\n",
      "[CV] n_estimators=10, max_depth=20 ...................................\n",
      "[CV] n_estimators=10, max_depth=20 ...................................\n",
      "[CV] n_estimators=100, max_depth=20 ..................................\n",
      "[CV]  n_estimators=10, max_depth=20, score=0.7402471750176441, total=   1.7s\n",
      "[CV] n_estimators=100, max_depth=20 ..................................\n",
      "[CV]  n_estimators=10, max_depth=20, score=0.7366480244129197, total=   1.8s\n",
      "[CV] n_estimators=100, max_depth=20 ..................................\n",
      "[CV]  n_estimators=10, max_depth=20, score=0.7493637874207784, total=   1.9s\n",
      "[CV] n_estimators=200, max_depth=20 ..................................\n",
      "[CV]  n_estimators=100, max_depth=20, score=0.7475293344987161, total=  11.5s\n",
      "[CV] n_estimators=200, max_depth=20 ..................................\n",
      "[CV]  n_estimators=100, max_depth=20, score=0.7379799315107213, total=  13.5s\n",
      "[CV] n_estimators=200, max_depth=20 ..................................\n",
      "[CV]  n_estimators=100, max_depth=20, score=0.7379607140932075, total=  13.1s\n",
      "[CV] n_estimators=1000, max_depth=20 .................................\n",
      "[CV]  n_estimators=200, max_depth=20, score=0.7370190450369098, total=  23.3s\n",
      "[CV] n_estimators=1000, max_depth=20 .................................\n",
      "[CV]  n_estimators=200, max_depth=20, score=0.7335064250142778, total=  18.1s\n",
      "[CV] n_estimators=1000, max_depth=20 .................................\n",
      "[CV]  n_estimators=200, max_depth=20, score=0.7433591185458266, total=  18.6s\n",
      "[CV] n_estimators=1200, max_depth=20 .................................\n",
      "[CV]  n_estimators=1000, max_depth=20, score=0.735337602686366, total= 1.3min\n",
      "[CV] n_estimators=1200, max_depth=20 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=1000, max_depth=20, score=0.7346614267045284, total= 1.3min\n",
      "[CV] n_estimators=1200, max_depth=20 .................................\n",
      "[CV]  n_estimators=1000, max_depth=20, score=0.7446808318257195, total= 1.3min\n",
      "[CV] n_estimators=1500, max_depth=20 .................................\n",
      "[CV]  n_estimators=1200, max_depth=20, score=0.736618176466816, total= 1.5min\n",
      "[CV] n_estimators=1500, max_depth=20 .................................\n",
      "[CV]  n_estimators=1200, max_depth=20, score=0.7339143880940687, total= 1.5min\n",
      "[CV] n_estimators=1500, max_depth=20 .................................\n",
      "[CV]  n_estimators=1200, max_depth=20, score=0.7441656700801085, total= 1.5min\n",
      "[CV] n_estimators=10, max_depth=100 ..................................\n",
      "[CV]  n_estimators=10, max_depth=100, score=0.7910049608732277, total=   3.7s\n",
      "[CV] n_estimators=10, max_depth=100 ..................................\n",
      "[CV]  n_estimators=10, max_depth=100, score=0.8021557999327927, total=   3.6s\n",
      "[CV] n_estimators=10, max_depth=100 ..................................\n",
      "[CV]  n_estimators=10, max_depth=100, score=0.8060461037315725, total=   3.7s\n",
      "[CV] n_estimators=100, max_depth=100 .................................\n",
      "[CV]  n_estimators=1500, max_depth=20, score=0.7350922969707675, total= 1.9min\n",
      "[CV] n_estimators=100, max_depth=100 .................................\n",
      "[CV]  n_estimators=100, max_depth=100, score=0.8300176475378054, total=  40.8s\n",
      "[CV] n_estimators=100, max_depth=100 .................................\n",
      "[CV]  n_estimators=1500, max_depth=20, score=0.7355716791000532, total= 1.9min\n",
      "[CV] n_estimators=200, max_depth=100 .................................\n",
      "[CV]  n_estimators=100, max_depth=100, score=0.832827313241493, total=  45.3s\n",
      "[CV] n_estimators=200, max_depth=100 .................................\n",
      "[CV]  n_estimators=100, max_depth=100, score=0.83996679816157, total=  43.9s\n",
      "[CV] n_estimators=200, max_depth=100 .................................\n",
      "[CV]  n_estimators=1500, max_depth=20, score=0.7457261929540316, total= 2.2min\n",
      "[CV] n_estimators=1000, max_depth=100 ................................\n",
      "[CV]  n_estimators=200, max_depth=100, score=0.8338005447625739, total= 1.5min\n",
      "[CV] n_estimators=1000, max_depth=100 ................................\n",
      "[CV]  n_estimators=200, max_depth=100, score=0.8324364278059747, total= 1.5min\n",
      "[CV] n_estimators=1000, max_depth=100 ................................\n",
      "[CV]  n_estimators=200, max_depth=100, score=0.8404685658995007, total= 1.5min\n",
      "[CV] n_estimators=1200, max_depth=100 ................................\n",
      "[CV]  n_estimators=1000, max_depth=100, score=0.8306033842393625, total= 6.2min\n",
      "[CV] n_estimators=1200, max_depth=100 ................................\n",
      "[CV]  n_estimators=1000, max_depth=100, score=0.8359300038428158, total= 6.2min\n",
      "[CV] n_estimators=1200, max_depth=100 ................................\n",
      "[CV]  n_estimators=1000, max_depth=100, score=0.8420609159924116, total= 6.1min\n",
      "[CV] n_estimators=1500, max_depth=100 ................................\n",
      "[CV]  n_estimators=1200, max_depth=100, score=0.8295702680423753, total= 7.1min\n",
      "[CV] n_estimators=1500, max_depth=100 ................................\n",
      "[CV]  n_estimators=1200, max_depth=100, score=0.8354120806759565, total= 6.7min\n",
      "[CV] n_estimators=1500, max_depth=100 ................................\n",
      "[CV]  n_estimators=1200, max_depth=100, score=0.8412378027523569, total= 7.1min\n",
      "[CV] n_estimators=10, max_depth=None .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 20.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=10, max_depth=None, score=0.779777130422683, total=   3.8s\n",
      "[CV] n_estimators=10, max_depth=None .................................\n",
      "[CV]  n_estimators=10, max_depth=None, score=0.7824666353903412, total=   3.8s\n",
      "[CV] n_estimators=10, max_depth=None .................................\n",
      "[CV]  n_estimators=10, max_depth=None, score=0.7868018987619609, total=   3.9s\n",
      "[CV] n_estimators=100, max_depth=None ................................\n",
      "[CV]  n_estimators=100, max_depth=None, score=0.8293775544172906, total=  36.8s\n",
      "[CV] n_estimators=100, max_depth=None ................................\n",
      "[CV]  n_estimators=100, max_depth=None, score=0.830196336087254, total=  37.0s\n",
      "[CV] n_estimators=100, max_depth=None ................................\n",
      "[CV]  n_estimators=100, max_depth=None, score=0.8372422535441857, total=  37.3s\n",
      "[CV] n_estimators=200, max_depth=None ................................\n",
      "[CV]  n_estimators=1500, max_depth=100, score=0.828840721720178, total= 8.8min\n",
      "[CV] n_estimators=200, max_depth=None ................................\n",
      "[CV]  n_estimators=1500, max_depth=100, score=0.8357357561466808, total= 8.5min\n",
      "[CV] n_estimators=200, max_depth=None ................................\n",
      "[CV]  n_estimators=200, max_depth=None, score=0.8285915062978141, total= 1.2min\n",
      "[CV] n_estimators=1000, max_depth=None ...............................\n",
      "[CV]  n_estimators=200, max_depth=None, score=0.8367693133794795, total= 1.2min\n",
      "[CV] n_estimators=1000, max_depth=None ...............................\n",
      "[CV]  n_estimators=200, max_depth=None, score=0.8419446492774942, total= 1.3min\n",
      "[CV] n_estimators=1000, max_depth=None ...............................\n",
      "[CV]  n_estimators=1500, max_depth=100, score=0.8415617098658871, total= 8.4min\n",
      "[CV] n_estimators=1200, max_depth=None ...............................\n",
      "[CV]  n_estimators=1000, max_depth=None, score=0.8391384073231514, total= 6.1min\n",
      "[CV] n_estimators=1200, max_depth=None ...............................\n",
      "[CV]  n_estimators=1000, max_depth=None, score=0.828243358848361, total= 6.2min\n",
      "[CV] n_estimators=1200, max_depth=None ...............................\n",
      "[CV]  n_estimators=1000, max_depth=None, score=0.845047830247177, total= 6.4min\n",
      "[CV] n_estimators=1500, max_depth=None ...............................\n",
      "[CV]  n_estimators=1200, max_depth=None, score=0.8299515468746669, total= 7.6min\n",
      "[CV] n_estimators=1500, max_depth=None ...............................\n",
      "[CV]  n_estimators=1200, max_depth=None, score=0.8368344814012191, total= 7.6min\n",
      "[CV] n_estimators=1500, max_depth=None ...............................\n",
      "[CV]  n_estimators=1200, max_depth=None, score=0.8425003297656873, total= 8.0min\n",
      "[CV]  n_estimators=1500, max_depth=None, score=0.8288900047453721, total= 9.5min\n",
      "[CV]  n_estimators=1500, max_depth=None, score=0.8370016608380336, total= 9.8min\n",
      "[CV]  n_estimators=1500, max_depth=None, score=0.8442140401859133, total= 9.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed: 47.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed: 47.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=<generator object _BaseKFold.split at 0x7f6f32427308>,\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [10, 100, 200, 1000, 1200, 1500], 'max_depth': [20, 100, None]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_weighted', verbose=8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = StratifiedKFold().split(X_train_features, y_train)\n",
    "\n",
    "param_space = {\n",
    "    'n_estimators': [10, 100, 200, 1000, 1200, 1500],\n",
    "    'max_depth': [20, 100, None],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(RandomForestClassifier(), param_space, scoring='f1_weighted', cv=cv, verbose=8, n_jobs=-1)\n",
    "gs.fit(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.97      0.90      5305\n",
      "          1       0.92      0.60      0.72      2482\n",
      "\n",
      "avg / total       0.86      0.85      0.85      7787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = gs.predict(X_valid_features)\n",
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fasttext(df, path):\n",
    "    with open(path, 'w+') as f:\n",
    "        for _, row in df.iterrows():\n",
    "            f.write('{} {}\\n'.format(row['y'], row['Description']))\n",
    "            \n",
    "            \n",
    "save_fasttext(train_df, '/tmp/train.csv')\n",
    "save_fasttext(valid_df, '/tmp/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./fasttext supervised -input /tmp/train.csv\\\n",
    "    -output /tmp/model\\\n",
    "    -verbose 10\\\n",
    "    -dim 300\\\n",
    "    -retrain\\\n",
    "    -pretrainedVectors cc.hy.300.vec\\\n",
    "    -epoch 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build predictions on a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./fasttext predict /tmp/model.bin /tmp/test.csv  > /tmp/pred.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calc metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      " __label__0       0.91      0.92      0.91      5305\n",
      " __label__1       0.82      0.79      0.81      2482\n",
      "\n",
      "avg / total       0.88      0.88      0.88      7787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = pd.read_csv('/tmp/pred.csv', header=None, names=['y'])['y'].values\n",
    "print(classification_report(valid_df['y'].values, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
